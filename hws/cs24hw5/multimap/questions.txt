Multimap Caching Performance
============================

b)  Output of mmperf:
Testing multimap performance:  300000 pairs, 1000000 probes, random keys.
Adding 300000 pairs to multimap.  Keys in range [0, 50), values in range [0, 1000).
Probing multimap 1000000 times.  Keys in range [0, 50), values in range [0, 1000).
Total hits:  997144/1000000 (99.7%)
Total wall-clock time:  15.77 seconds		us per probe:  15.775 us

Testing multimap performance:  300000 pairs, 1000000 probes, incrementing keys.
Adding 300000 pairs to multimap.  Keys in range [0, 50), values in range [0, 1000).
Probing multimap 1000000 times.  Keys in range [0, 50), values in range [0, 1000).
Total hits:  997715/1000000 (99.8%)
Total wall-clock time:  36.73 seconds		us per probe:  36.734 us

Testing multimap performance:  300000 pairs, 1000000 probes, decrementing keys.
Adding 300000 pairs to multimap.  Keys in range [0, 50), values in range [0, 1000).
Probing multimap 1000000 times.  Keys in range [0, 50), values in range [0, 1000).
Total hits:  997325/1000000 (99.7%)
Total wall-clock time:  36.30 seconds		us per probe:  36.304 us

Testing multimap performance:  15000000 pairs, 1000000 probes, random keys.
Adding 15000000 pairs to multimap.  Keys in range [0, 100000), values in range [0, 50).
Probing multimap 1000000 times.  Keys in range [0, 100000), values in range [0, 50).
Total hits:  949586/1000000 (95.0%)
Total wall-clock time:  5.91 seconds		us per probe:  5.910 us

Testing multimap performance:  100000 pairs, 50000 probes, incrementing keys.
Adding 100000 pairs to multimap.  Keys in range [0, 100000), values in range [0, 50).
Probing multimap 50000 times.  Keys in range [0, 100000), values in range [0, 50).
Total hits:  976/50000 (2.0%)
Total wall-clock time:  151.45 seconds		us per probe:  3028.907 us

Testing multimap performance:  100000 pairs, 50000 probes, decrementing keys.
Adding 100000 pairs to multimap.  Keys in range [0, 100000), values in range [0, 50).
Probing multimap 50000 times.  Keys in range [0, 100000), values in range [0, 50).
Total hits:  980/50000 (2.0%)
Total wall-clock time:  146.06 seconds		us per probe:  2921.237 us

./mmperf  917.37s user 0.72s system 99% cpu 15:21.32 total


c)  Explanation of tests:
We can observe that the first three tests have a relatively small range of
key values and a relatively large range of value values. This means that the
multimap trees created by these tests have a limited number of nodes: 50, to
be precise. However, since we are creating so many key-value pairs in these
tests, and given our relatively small range of keys, we will find that each
node will most likely have a lot of values associated with it, because we will
get a lot of repeated keys. This means that the linked list of values that each
node holds will most likely be relatively long. Thus, when we are probing for
pairs in this tree, it will take a relatively short time to find the key, but
finding the value may take some time because we have to iterate through the
linked list of values.

We can then observe that the last three tests have a relatively large range of
key values and a relatively small range of value values. This means that the
multimap trees created by these tests will have many more nodes than the
previous three tests; our limit is now 100000! However, since our range of
keys is so large, we will find that the linked list of values for each node will
be relatively short, because we won't get a lot of repeated keys. Thus, when we
are probing for pairs in this tree, it will take a relatively long time to find
the key (because there are so many), but finding the value should be a quick
operation.

In terms of the code, this means that for the first three tests, the while loop
find_mm_node() (finds the node with associated with a given key, starting from
the root) will most likely be the most time consuming operation. For the
last three tests, the while loop in mm_contains_pair() (starting from a node,
goes through its linked list and looks for a value) will most likely be
the most time consuming operation.


e)  Explanation of your optimizations:

Optimization 1: convert values linked-list to values array
    - The cache-performance issue I address is that, in the original
      implementation, the values were not stored in a contiguous chunk of
      memory. This creates a large number of cache misses when we access the
      values because they are scattered throughout main memory, which makes it
      unlikely for entire linked lists of values to be cached.
    - My optimization was to convert the linked list of value structs into a
      simple integer array. To do this, I added three new members to
      the multimap_node struct: curr_size, max_size, and values. I made
      curr_size and max_size unsigned shorts in order to reduce the size of
      multimap_nodes (slightly improves performance). We need curr_size to tell
      us where to insert the next value, and we need max_size to keep track of
      the overall size of the array. If the array ever gets too full, I simply
      resize it by copying over the values to a newly allocated array which
      has double the size. Codewise, I added one method to implement this
      optimization: resize_values(). This method just resizes the array of
      values for a given node. Of course, I had to refactor the code as well.
    - This mitigates the problem because it ensures that, for each key, all the
      values are stored in a contiguous chunk of memory. This makes it much more
      likely that the entire value arrays are cached, or at least large parts
      of them are cached, which reduces cache misses and makes our program
      faster.

Optimization 2: use slab allocation for allocating multimap_nodes
    - The cache-performance issue I address is that, in the original
      implementation, the multimap_nodes were not stored in a contiguous chunk
      of memory (they were just allocated using malloc()). Thus, when we
      traverse the tree's nodes, we get a large number of cache misses because
      the nodes are scattered throughout main memory, which makes it unlikely
      that a lot of nodes will be cached at the same time.
    - My optimization was to use slab allocation for allocating multimap_nodes.
      This means that I create an array of memory banks in which I allocate
      all the multimap nodes, to ensure that large runs of them are allocated
      contiguously. I also keep a couple of helper variables to help me manage
      this array of memory banks. Basically, these variables let me know when
      I need to new memory bank to the array, and index the location in memory
      where I should allocate the next multimap_node. I use an array of memory
      banks so that if I ever run out of memory, I can just add another memory
      bank to the array. I was thinking of using realloc() with one big memory
      block, but that creates because each multimap_node stores pointers to
      other multimap_nodes. Codewise, I added two new methods to implement this
      optimization: resize_memory_banks() and add_memory_bank(). The former
      method makes the array of memory banks bigger, and the latter adds another
      memory bank to the array of memory banks.
    - This mitigates the problem because it ensures that large runs of nodes are
      stored in contiguous chunks of memory. This makes it so that large runs
      of nodes are cached, which reduces the number of cache misses we get when
      we traverse the tree. In other words, since many nodes get stored
      in contiguous chunks of memory, it is easier to cache them, which
      reduces cache misses.


f)  Output of ommperf:
Testing multimap performance:  300000 pairs, 1000000 probes, random keys.
Adding 300000 pairs to multimap.  Keys in range [0, 50), values in range [0, 1000).
Probing multimap 1000000 times.  Keys in range [0, 50), values in range [0, 1000).
Total hits:  997144/1000000 (99.7%)
Total wall-clock time:  0.42 seconds		us per probe:  0.419 us

Testing multimap performance:  300000 pairs, 1000000 probes, incrementing keys.
Adding 300000 pairs to multimap.  Keys in range [0, 50), values in range [0, 1000).
Probing multimap 1000000 times.  Keys in range [0, 50), values in range [0, 1000).
Total hits:  997715/1000000 (99.8%)
Total wall-clock time:  0.46 seconds		us per probe:  0.456 us

Testing multimap performance:  300000 pairs, 1000000 probes, decrementing keys.
Adding 300000 pairs to multimap.  Keys in range [0, 50), values in range [0, 1000).
Probing multimap 1000000 times.  Keys in range [0, 50), values in range [0, 1000).
Total hits:  997325/1000000 (99.7%)
Total wall-clock time:  0.44 seconds		us per probe:  0.443 us

Testing multimap performance:  15000000 pairs, 1000000 probes, random keys.
Adding 15000000 pairs to multimap.  Keys in range [0, 100000), values in range [0, 50).
Probing multimap 1000000 times.  Keys in range [0, 100000), values in range [0, 50).
Total hits:  949586/1000000 (95.0%)
Total wall-clock time:  0.53 seconds		us per probe:  0.533 us

Testing multimap performance:  100000 pairs, 50000 probes, incrementing keys.
Adding 100000 pairs to multimap.  Keys in range [0, 100000), values in range [0, 50).
Probing multimap 50000 times.  Keys in range [0, 100000), values in range [0, 50).
Total hits:  976/50000 (2.0%)
Total wall-clock time:  4.25 seconds		us per probe:  84.956 us

Testing multimap performance:  100000 pairs, 50000 probes, decrementing keys.
Adding 100000 pairs to multimap.  Keys in range [0, 100000), values in range [0, 50).
Probing multimap 50000 times.  Keys in range [0, 100000), values in range [0, 50).
Total hits:  980/50000 (2.0%)
Total wall-clock time:  4.32 seconds		us per probe:  86.488 us

./ommperf  31.94s user 0.35s system 98% cpu 32.917 total
